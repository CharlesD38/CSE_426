{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\\tableofcontents \n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the training examples be $x^{(1)} = [1,0]^T$, $x^{(2)} = [1,1]^T$, $y^{(1)} = 10$, $y^{(2)} = -10$. Let the parameters of the linear regression model be $\\theta = [-1, 3]^T$. Calculate the likelihood of the training data under the model two steps: 1) write down the general likelihood equation for linear regression (assuming $\\sigma = 1$); 2) plug in the data and parameters to calculate the likelihood (no need to get the value but express your results using exp without the $x$, $y$, and $\\theta$ symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The genral likelihood equation is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta;\\{ x^{(i)}, y^{(i)}\\}_{i=1}^m) &= \\prod_{i=1}^mPr(y^{(i)} | x^{(i)}: \\theta) \\\\\n",
    "\\\\\n",
    "&= \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right)^m \\exp \\left\\{-\\frac{1}{\\sigma^2} \\sum_{i=1}^m(y^{(i)} - \\theta^Tx^{(i)})^2 \\right\\}\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Plugging in our training data and assumptions we arrive at:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta;\\{ x^{(i)}, y^{(i)}\\}_{i=1}^m) &= \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2 \\exp \\left\\{\n",
    "-\\left[\n",
    "\\left(\n",
    "10-[-1,3]\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\end{bmatrix}\n",
    "\\right)^2\n",
    "+\n",
    "\\left(\n",
    "-10-[-1,3]\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\end{bmatrix}\n",
    "\\right)^2\n",
    "\\right]\n",
    "\\right\\}\\\\\n",
    "\\\\\n",
    "&= \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)^2 \\exp \\left\\{-265 \\right\\}\\\\\n",
    "\\end{split}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The likelihood function of linear regression in the lecture note assumes that the variance $\\sigma^2$ is the same for each training example. Now assume that the $i$ -th training example has a specific variance $\\sigma_i^2$, where the variances for two different examples can be different. Prove that the likelihood of the $i$ training example goes to $0$ as $\\sigma_i$ $\\rightarrow$ $\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by restating the likelihood formulation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta;\\{ x^{(i)}, y^{(i)}\\}_{i=1}^m) &= \\prod_{i=1}^mPr(y^{(i)} | x^{(i)}: \\theta)\\\\\n",
    "\\\\\n",
    "&= \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right)^m \\exp \\left\\{-\\frac{1}{\\sigma^2} \\sum_{i=1}^m(y^{(i)} - \\theta^Tx^{(i)})^2 \\right\\}\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where the product is due to the independence of training examples. Furthere more, adding Gaussian noise to the error terms in the linear approximation where the following is true,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\text{(1)}& \\ \\ \\ \\ \\epsilon^{(i)} \\sim N(0,\\sigma^2)\\\\\n",
    "\\\\\n",
    "\\text{(2)}& \\ \\ \\ \\ y^{(i)} = \\theta^Tx^{(i)} + \\epsilon^{(i)}\\\\\n",
    "\\\\\n",
    "\\text{(3)}& \\ \\ \\ \\ Pr(\\epsilon^{(i)};0,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left\\{-\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2}\\right\\}\\\\\n",
    "\\\\\n",
    "\\text{(4)}& \\ \\ \\ \\ Pr(y^{(i)} | x^{(i)}: \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left\\{-\\frac{(y^{(i)} - \\theta^Tx^{(i)} )^2}{\\sigma^2} \\right\\}\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Making one small adjustment to equation four for the purposes of $\\sigma^2$ being different for all training examples:\n",
    "$$\n",
    "Pr(y^{(i)} | x^{(i)}: \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_i} \\exp \\left\\{-\\frac{(y^{(i)} - \\theta^Tx^{(i)} )^2}{\\sigma_i^2} \\right\\}\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "Thus we can see the following:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\theta;\\{ x^{(i)}, y^{(i)}\\}_{i=1}^m) &= \\prod_{i=1}^mPr(y^{(i)} | x^{(i)}: \\theta) \\\\\n",
    "\\\\\n",
    "&= (Pr(y^{(1)} | x^{(1)}: \\theta))(Pr(y^{(2)} | x^{(2)}: \\theta)) \\ \\text{...} \\ (Pr(y^{(m)} | x^{(m)}: \\theta))\\\\\n",
    "\\\\\n",
    "&= \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_1} \\exp \\left\\{-\\frac{(y^{(1)} - \\theta^Tx^{(1)} )^2}{\\sigma_1^2} \\right\\}\\right)\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_2} \\exp \\left\\{-\\frac{(y^{(2)} - \\theta^Tx^{(2)} )^2}{\\sigma_2^2} \\right\\}\\right) \\ ... \\ \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma_m} \\exp \\left\\{-\\frac{(y^{(m)} - \\theta^Tx^{(m)} )^2}{\\sigma_m^2} \\right\\}\\right)\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus for one single likelihood, if $\\sigma_i \\to \\infty$, we can see that $\\frac{1}{\\sqrt{2\\pi}\\sigma_i} \\to \\frac{1}{\\infty} \\to 0$ and thus the likelihood of the trainging example $\\to 0$ due to multiplicative law of $0$. From the above formulation we can also see that the general likelihood will also approach $0$ if any $\\sigma_i \\to \\infty$ $\\blacksquare$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prove that $\\ \\frac{\\partial}{\\partial z} \\log(\\sigma(-z)) = -\\sigma(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We state that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial z} \\log(\\sigma(-z)) = -\\sigma(z)\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "We state the following truth: $1-\\sigma(z) = \\sigma(-z)$ thus, $-\\sigma(z) = \\sigma(-z) - 1$\n",
    "\n",
    "Following this, we derive the following:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial z} \\log(\\sigma(-z)) &= -\\sigma(z) \\\\\n",
    "\\\\\n",
    "\\frac{1}{\\sigma(-z)} \\left(\\frac{\\partial}{\\partial z}\\sigma(-z) \\right) & = \\sigma(-z) - 1 \\ \\ \\ \\ \\ \\ \\ \\text{Derivative of } \\log(g(x))\\\\\n",
    "\\\\\n",
    "e^z + 1 \\left(\\frac{\\partial}{\\partial z}\\frac{1}{e^z+1}\\right) &= \\frac{1}{e^z + 1} - 1\\\\\n",
    "\\\\\n",
    "e^z + 1 \\left(-\\frac{\\frac{\\partial}{\\partial z}[e^z+1]}{(e^z+1)^2}\\right) &= \\frac{1}{e^z + 1} - \\frac{e^z + 1}{e^z + 1}\\ \\ \\ \\ \\ \\ \\ \\text{Reciprocal Rule }\\\\\n",
    "\\\\\n",
    "e^z + 1 \\left(-\\frac{e^z}{(e^z+1)^2}\\right) &= -\\frac{e^z}{e^z + 1}\\ \\ \\ \\ \\ \\ \\ \\text{Derivative of } e^x\\\\\n",
    "\\\\\n",
    "-\\frac{e^z}{e^z+1} &= -\\frac{e^z}{e^z + 1} \\ \\ \\ \\ \\ \\ \\ \\ \\blacksquare\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the training examples be $x^{(1)} = [1,0]^T$, $x^{(2)} = [1,1]^T$, $y^{(1)} = 1$, $y^{(2)} = 0$. Evaluate the log-likelihood of a logistic regression with parameter $\\theta = [-1, 3]^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\log L(\\theta) &= \\sum_{i=1}^m \\{y^{(i)}\\log\\sigma(z^{(i)})+(1-y^{(i)})\\log\\sigma(-z^{(i)})\\}\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Where $\\sigma(z) = \\frac{1}{1+e^{-z}}$ and $z^{(i)} = \\theta^Tx^{(i)}$. Thus,\n",
    "$$\n",
    "\\begin{split}\n",
    "z^1 &= [-1,3]\\begin{bmatrix}1\\\\0\\end{bmatrix} = -1\\\\\n",
    "\\\\\n",
    "z^2 &= [-1,3]\\begin{bmatrix}1\\\\1\\end{bmatrix} = 2\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sigma(z^{(1)}) &= \\frac{1}{1+e^{-z^{(1)}}} = \\frac{1}{1+e^{1}}\\\\\n",
    "\\\\\n",
    "\\sigma(-z^{(1)}) &= \\frac{1}{1+e^{z^{(1)}}} = \\frac{1}{1+e^{-1}}\\\\\n",
    "\\\\\n",
    "\\sigma(z^{(2)}) &= \\frac{1}{1+e^{-z^{(2)}}} = \\frac{1}{1+e^{-2}}\\\\\n",
    "\\\\\n",
    "\\sigma(-z^{(2)}) &= \\frac{1}{1+e^{z^{(2)}}} = \\frac{1}{1+e^{2}}\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputing these values,\n",
    "$$\n",
    "\\begin{split}\n",
    "\\log L(\\theta) &= \\sum_{i=1}^m \\{y^{(i)}\\log\\sigma(z^{(i)})+(1-y^{(i)})\\log\\sigma(-z^{(i)})\\}\\\\\n",
    "\\\\\n",
    "&= \\left[y^{(1)}\\log\\sigma(z^{(1)})+(1-y^{(1)})\\log\\sigma(-z^{(1)}) \\right] + \\left[(y^{(2)}\\log\\sigma(z^{(2)})+(1-y^{(2)})\\log\\sigma(-z^{(2)})\\right]\\\\\n",
    "\\\\\n",
    "&= \\log\\frac{1}{1+e^{1}} + \\log\\frac{1}{1+e^{2}}\\\\\n",
    "\\\\\n",
    "&= \\log\\left[\\left(\\frac{1}{1+e^{1}}\\right)\\left(\\frac{1}{1+e^{2}}\\right)\\right]\\\\\n",
    "\\\\\n",
    "&= \\log\\frac{1}{1+e^{3}+e^{2}+e^{1}}\\\\\n",
    "\\\\\n",
    "&= -3.4402\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Graduate only) Newton method for multi-class logistic regression requires the Hessian matrix that contains second-order derivatives. Let $z_j = \\theta_j^Tx$. Derive the second-order partial derivative of the log of the softmax output $\\phi_j (x) = \\frac{\\exp(z_j)}{\\sum_{l=1}^k \\exp(z_l)}$ for class j. Formally, prove that $\\frac{\\partial^2log\\phi_j (x)}{\\partial\\theta_j\\partial\\theta_i} = -\\phi_j(\\delta_{ij} - \\phi_i)XX^T \\in \\mathbb{R}^{n\\text{x}n}$, where $n$ is the dimension of $x$, and $\\delta_{ij} = \\mathbb{1}[i = j] = 1$ if $i = j$ and $0$ otherwise. (Hints: start from the gradient of $\\log\\phi_j \\text{ w.r.t } \\theta_j$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial\\theta_j} \\log(\\phi_j(x)) &= \\frac{\\partial}{\\partial\\theta_j}\\log\\left[\\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\right]\\\\\n",
    "\\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_j}\\left[\\log(\\exp(\\theta_j^TX)) - \\log(\\sum_{l=1}^k\\exp(\\theta_l^TX)) \\right]\\\\\n",
    "\\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_j}\\log(\\exp(\\theta_j^TX)) - \\frac{\\partial}{\\partial\\theta_j}\\log(\\sum_{l=1}^k\\exp(\\theta_l^TX))\\\\\n",
    "\\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_j}\\theta_j^TX - \\frac{1}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\frac{\\partial}{\\partial\\theta_j}\\sum_{l=1}^k\\exp(\\theta_l^TX)\\\\\n",
    "\\\\\n",
    "&= \\mathbb{1}X - \\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\frac{\\partial}{\\partial\\theta_j}\\theta_j^TX\\\\\n",
    "\\\\\n",
    "&= [\\mathbb{1} - \\phi_j]X\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_i} \\log(\\phi_j(x)) &= \\frac{\\partial}{\\partial\\theta_i}[\\mathbb{1} - \\phi_j]X\\\\\n",
    "\\\\\n",
    "&= \\frac{\\partial}{\\partial\\theta_i}\\left[\\mathbb{1} - \\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\right]X\\\\\n",
    "\\\\\n",
    "&= \\left[\\frac{\\partial}{\\partial\\theta_i}\\mathbb{1} - \\frac{\\partial}{\\partial\\theta_i}\\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\right]X\\\\\n",
    "\\\\\n",
    "&= \\left[- \\left(\\frac{\\frac{\\partial}{\\partial\\theta_i}\\exp(\\theta_j^TX)\\sum_{l=1}^k\\exp(\\theta_l^TX) - \\frac{\\partial}{\\partial\\theta_i}  \\sum_{l=1}^k\\exp(\\theta_l^TX)\\exp(\\theta_j^TX)}{\\left(\\sum_{l=1}^k\\exp(\\theta_l^TX)\\right)^2}\\right)\\right]X\\\\\n",
    "\\\\\n",
    "&= \\left[- \\left(\\frac{\\delta_{ij}X\\exp(\\theta_j^TX)\\sum_{l=1}^k\\exp(\\theta_l^TX) - \\exp(\\theta_i^TX)X\\exp(\\theta_j^TX)}{\\left(\\sum_{l=1}^k\\exp(\\theta_l^TX)\\right)^2}\\right)\\right]X\\\\\n",
    "\\\\\n",
    "&= \\left[- \\left(\\delta_{ij}X\\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\frac{\\sum_{l=1}^k\\exp(\\theta_l^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)} - X\\frac{\\exp(\\theta_j^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\\frac{\\exp(\\theta_i^TX)}{\\sum_{l=1}^k\\exp(\\theta_l^TX)}\n",
    "\\right)\\right]X\\\\\n",
    "\\\\\n",
    "&= (-\\delta_{ij}\\phi_jX - \\phi_j\\phi_iX)X\\\\\n",
    "\\\\\n",
    "&= -\\phi_j(\\delta_{ij} - \\phi_i)XX^T \\blacksquare\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
