{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Neural network architecture: given input feature vectors $x^{(i)} \\in \\mathbb{R}^5$, we want to predict one out of three classes for each input. Design a neural network with 1 input layer, 1 hidden layer, and 1 output layer. Specify the following architecture parameters:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* number of neurons of each layer (you have to select one for the hidden layer as well);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* activation function $g^{[l]}$ for each layer $ l = 1, \\ 2$. Make sure to think about which activation function is suitable for predicting one out of more than two classes. You may need to refer back to the logistic regression note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a diagram for this. I think it easier to understand and more concise. I will list the neurons as this is not in the diagram but I thought may be obvious. Just to be safe $n^{[0]} = 5$, $n^{[1]} = 10$, $n^{[0]} = 3$. The input and hidden layer are not counting the bias term. I made $n^{[1]}$ as just $2(n^{[0]})$ or $2(\\text{number of features})$. Output layer must be three as we have three classes and are identifying which trainning example is part of which class.\n",
    "\n",
    "This leads to $g^{[l]}$. Since we are classifying multiclass we cannot use sigmoid becasue it is not binary. For the hidden layer I chose the trusty ReLU$(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My Neural Network](Q1Dia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the above network, list the weight and bias parameters $((W^{[l]}, \\ b^{[l]}) \\text{ for } l = 1, \\ 2)$ necessary to define the neural network. Make sure you specify the number of rows and columns of each such parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My Neural Network (Weights)](Q2Dia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * Given $B$ training examples, feeding them through an MLP will generate $B$ column activation vectors at the output layer, collected in the matrix $A \\in \\mathbb{R}^{n[L] \\times B}$. Let $A^{[L-1]}$ be the matrix $B$ activation vectors at the penultimate (second last) layer. Let $Y \\in \\{ 0, 1 \\}^{n[L] \\times B}$ be the ground truth label matrix, with the $k$-th column $y^{(k)}$ being the one-hot label vector for the k-th training example. Prove that $$\\frac{\\partial J}{\\partial W^{[L]}} = \\frac{1}{B}(A^{[L]} - Y)(A^{[L-1]})^T \\in \\mathbb{R}^{n[L] \\times n[L-1]}$$ is indeed the average of the gradients of the loss function $J$ with respect to $W^{[L]}$ over the $B$ training examples. Your answer should contain the gradients for each of the $B$ training examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SGD method we can say the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "J(Y, A^{[L]}) &= -Y \\log A^{[L]} - (1 - Y) \\log (1 - A^{[L]})\\\\\n",
    "\\\\\n",
    "&= \\sum_{k=1}^B -y^{(k)} \\log a^{[L](k)} - (1 - y^{(k)}) \\log (1 - a^{[L](k)})\\\\\n",
    "\\\\\n",
    "z^{[L]} &= W^{[L]}a^{[L-1]} + b^{[L]}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J}{\\partial W^{[L]}} &= \\frac{\\partial J(Y,A^{[L]})}{\\partial z^{[L]}} \\times \\frac{\\partial z^{[L]}}{\\partial W^{[L]}}\\\\\n",
    "\\\\\n",
    "&=  \\sum_{k=1}^B (a^{[L](k)} - y^{(k)})(a^{[L-1](k)})^T\\\\\n",
    "\\\\\n",
    "&= \\frac{1}{B} \\sum_{k=1}^B (a^{[L](k)} - y^{(k)})(a^{[L-1](k)})^T \\quad \\text{(due to this being an SGD problem)}\\\\\n",
    "\\\\\n",
    "&= \\frac{1}{B}(A^{[L]} - Y)(A^{[L-1]})^T\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP can also be used for regression on multiple target values. Given training examples $\\{ x^{(i)}; y^{(i)} \\}_{i=1}^m$ where $y^{(i)} \\in \\mathbb{R}^k$ and $x^{(i)} \\mathbb{R}^n$. MSE loss will be appropriate for training an MLP to predict $\\hat{y}^{(i)}$, so that $\\hat{y}^{(i)}$ is as close as possible to $y^{(i)}$ for the input $x^{(i)}$. The MSE loss function will then be $$J = \\frac{1}{2m}\\sum_{i=1}^m \\lVert \\hat{y}^{(i)} - y^{(i)} \\rVert _2^2$$ where $\\lVert w \\rVert_2$ is the 2-norm of the vector $w$. Find the partial derivative of $J$ w.r.t $\\hat{y}^{(i)}$. IS the softmax activation function appropriate for regression MLP? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# * (Graduate only) This question is related to Project 3. In the function **explain** of the **NN** class, you will be asked to find $\\frac{\\partial z_c^{[L]}}{\\partial a^{[0]}}$, the gradient of $z_c^{[L]}$ w.r.t the input data $a^{[0]}$, where $z_c^{[L]} = W_c^{[L]} a^{[L-1]}$ and $W_c^{[L]}$ is the $c$-th row of the parameter matrix $W^{[L]}$, and $a^{[0]} \\in \\mathbb{R}^n$ is the input vector. Use the following hints to find $\\frac{\\partial z_c^{[L]}}{\\partial a^{[0]}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
