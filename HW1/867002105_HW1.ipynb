{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\tableofcontents \n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 {.unnumbered}\n",
    "\n",
    "Let the training examples be $x^{(1)} = (1,0.5,3)^T$, $x^{(2)} = (1,0.2,−1)^T$, $y^{(2)} = 5$, $y^{(2)} = 2$. Write the MSE loss function $J(\\theta)$ with the parameters $\\theta$ and the above training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by introducing the our cost function $J(\\theta)$ where:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "h(x; \\theta) &= \\langle\\theta^Tx\\rangle\\\\\n",
    "\\\\\n",
    "J(\\theta) &= \\frac{1}{m}\\sum_{i=1}^m\\frac{1}{2}(h(x^{(i)};\\theta) - y^{(i)})^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Introducing our vectors:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(\\theta) &= \\frac{1}{2} [\\frac{1}{2}(\\theta^Tx^{(1)} - 5)^2 + \\frac{1}{2}(\\theta^Tx^{(2)} - 2)^2] \\\\ \n",
    "\\\\\n",
    "& = \\frac{1}{4} [(\\theta_0 + \\frac{1}2\\theta_1 + 3\\theta_2 - 5)^2 + (\\theta_0 + \\frac{1}5\\theta_1 - \\theta_2 - 2)^2] \\\\\n",
    "\\\\\n",
    "J(\\theta) &= \\frac{1}{2}\\theta_0^2 +\\frac{7}{20}\\theta_0\\theta_1 + \\theta_0\\theta_2 + \\frac{29}{400}\\theta_1^2 + \\frac{13}{20}\\theta_1\\theta_2 + \\frac{5}{2}\\theta_2^2 -\\frac{7}{20}\\theta_0 - \\frac{29}{20}\\theta_1 - \\frac{13}{2}\\theta_2 +\\frac{29}{40}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 {.unnumbered}\n",
    "\n",
    "Derive the gradient (a 3 dimensional column vector) of $J(\\theta)$ with respect to $\\theta$. Need to derive the partial derivatives of $J(\\theta)$ with respect to each element of $\\theta$ explicitly. Then write the gradient vector in the form of a matrix-vector product plus a constant vector. We called this the “vectorized” gradient that will be useful for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\begin{bmatrix}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1}\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_2} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\theta_0^2 +\\frac{7}{20}\\theta_0\\theta_1 + \\theta_0\\theta_2 + \\frac{29}{400}\\theta_1^2 + \\frac{13}{20}\\theta_1\\theta_2 + \\frac{5}{2}\\theta_2^2 -\\frac{7}{20}\\theta_0 - \\frac{29}{20}\\theta_1 - \\frac{13}{2}\\theta_2 + \\frac{29}{40}\n",
    "$$\n",
    "\n",
    "Thus our columns vector is \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\begin{bmatrix}\n",
    "\\theta_0 + \\frac{7}{20}\\theta_1 + \\theta_2 -\\frac{7}{20}\\\\\n",
    "\\\\\n",
    "\\frac{7}{20}\\theta_0 + \\frac{29}{200}\\theta_1 + \\frac{13}{20}\\theta_2 -\\frac{29}{20}\\\\\n",
    "\\\\\n",
    "\\theta_0 + \\frac{13}{20}\\theta_1 + 5\\theta_2 - \\frac{13}{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now we can see that our Gradient Vector $= A\\theta - b$ where,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\theta &= \\begin{bmatrix}\n",
    "\\theta_0 \\\\ \n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "A &= \\begin{bmatrix}\n",
    "1 & \\frac{7}{20} & 1 \\\\ \n",
    "\\\\\n",
    "\\frac{7}{20} & \\frac{29}{200} & \\frac{13}{20} \\\\\n",
    "\\\\\n",
    "1 & \\frac{13}{20} & 5 \\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "b &= \\begin{bmatrix}\n",
    "\\frac{7}{20} \\\\ \n",
    "\\\\\n",
    "\\frac{29}{20} \\\\\n",
    "\\\\\n",
    "\\frac{13}{2} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 {.unnumbered}\n",
    "\n",
    "Use matrix calculus to derive the gradient of $J(\\theta)$ with respect to $\\theta$. The gradient must use the design matrix with the $i$-th rows being $(x^{(i)}), i = 1,2$, and the target value vector $y = [y^{(1)},y^{(2)}]$. Please don’t derive it based on partial derives as in the above question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by writing our $J(\\theta)$ in vector notation:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & \\frac{1}{2} & 3\\\\ \n",
    "\\\\\n",
    "1 & \\frac{1}{5} & -1\\\\\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\ \\ \\,\n",
    "\\theta = \\begin{bmatrix}\n",
    "\\theta_0 \\\\ \n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\end{bmatrix}\n",
    "\\ \\ \\ \\ \\ \\,\n",
    "y = \\begin{bmatrix}\n",
    "5\\\\ \n",
    "\\\\\n",
    "2\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We also know that our residuals, $r = X\\theta-y$, thus:\n",
    "\n",
    "$$\n",
    "r = X\\theta-y = \\begin{bmatrix}\n",
    "\\theta_0 + \\frac{1}{2}\\theta_1+3\\theta_2-5\\\\ \n",
    "\\\\\n",
    "\\theta_0 + \\frac{1}{5}\\theta_1-\\theta_2-2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the gradient of $J(\\theta)$ w.r.t $\\theta$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{\\partial}{\\partial \\theta}J(\\theta) &= \\frac{\\partial}{\\partial \\theta}\\frac{1}{2m}(X\\theta-y)^T(X\\theta-y)\\\\\n",
    "\\\\\n",
    "&= \\frac{1}{4}(2X^Tr)\\\\\n",
    "\\\\\n",
    "&=\\frac{1}{2}(X^Tr)\\\\\n",
    "\\\\\n",
    "&=\\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 + \\frac{1}{2}\\theta_1+3\\theta_2-5 \\\\ \n",
    "\\\\\n",
    "\\theta_0 + \\frac{1}{5}\\theta_1-\\theta_2-2\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "1  & 1 \\\\ \n",
    "\\\\\n",
    "\\frac{1}{2} & \\frac{1}{5} \\\\\n",
    "\\\\\n",
    "3 & -1\n",
    "\\end{bmatrix}\\\\\n",
    "\\\\\n",
    "&= \\begin{bmatrix}\n",
    "\\theta_0 + \\frac{7}{20}\\theta_1 + \\theta_2 -\\frac{7}{20}\\\\\n",
    "\\\\\n",
    "\\frac{7}{20}\\theta_0 + \\frac{29}{200}\\theta_1 + \\frac{13}{20}\\theta_2 -\\frac{29}{20}\\\\\n",
    "\\\\\n",
    "\\theta_0 + \\frac{13}{20}\\theta_1 + 5\\theta_2 - \\frac{13}{2}\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4 {.unnumbered}\n",
    "\n",
    "Use matrix calculus to find the gradient of the gradient $\\frac{\\partial J(\\theta)}{\\partial \\theta}$ . This is the so-called “second-order derivatives” or the Hessian matrix $H$. Show that this matrix is positive-semidefinite (PSD), defined as for any vector $u \\ \\epsilon \\ R^3$, $u^THu \\geq 0$. Note: the result is a 3-by-3 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling our gradient from Problem 3:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = 1/2(X^Tr)\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "r = X\\theta - y\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = 1/2(X^TX\\theta - y)\n",
    "$$\n",
    "\n",
    "To find our Hessian matrix we must take the second order derivation of $J(\\theta)$ w.r.t $\\theta$,\n",
    "\n",
    "$$\n",
    "H = \\frac{\\partial^2 J(\\theta)}{\\partial \\theta^2} = 1/2(X^TX)\n",
    "$$\n",
    "\n",
    "Plugging in our matrix $X$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "H = \\frac{\\partial^2 J(\\theta)}{\\partial \\theta^2} &= 1/2(X^TX)\\\\\n",
    "\\\\\n",
    "&= \\frac{1}{2}\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "\\\\\n",
    "\\frac{1}{2} & \\frac{1}{5} \\\\\n",
    "\\\\\n",
    "3 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 & \\frac{1}{2} & 3\\\\ \n",
    "\\\\\n",
    "1 & \\frac{1}{5} & -1\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{split}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We arrive at \n",
    "$$\n",
    "H = \n",
    "\\begin{bmatrix}\n",
    "1 & \\frac{7}{20} & 1 \\\\ \n",
    "\\\\\n",
    "\\frac{7}{20} & \\frac{29}{200} & \\frac{13}{20} \\\\\n",
    "\\\\\n",
    "1 & \\frac{13}{20} & 5 \\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "And we can clearly see that the matrix $H$ is symmetric. Thus to prove PSD we shall show all eigen values are $\\geq 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "|       |   $\\lambda_0$ |   $\\lambda_1$ |   $\\lambda_2$ |\n",
       "|:------|--------------:|--------------:|--------------:|\n",
       "| Value |             0 |        0.8108 |        5.3342 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''importing numpy'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown as md\n",
    "'''creating the matrix'''\n",
    "H = np.array([\n",
    "    [1, 7/20, 1],\n",
    "    [7/20, 29/200, 13/20],\n",
    "    [1, 13/20, 5]\n",
    "])\n",
    "'''Eigen-Value Calculations'''\n",
    "lambdas = pd.DataFrame([round(np.linalg.eigh(H)[0][i],4) for i in range(3)], index=['$\\lambda_{}$'. format(i) for i in range(3)], columns=['Value'])\n",
    "\n",
    "'''Printing the Value'''\n",
    "md(lambdas.T.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have proven $H$ is PSD since a square symmetrical matrix is PSD iff all $\\lambda \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5 {.unnumbered}\n",
    "\n",
    "(Graduate only) Stochastic gradient descent samples one training example to calculate the gradient vector. What needs to be changed in the gradient found in Question 3 to find a stochastic gradient? Then find the two gradients with $(x^{(1)},y^{(1)})$ and $(x^{(2)}, y^{(2)})$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 finds the gradient descent using all of the training data. This is what needs to be changed. Using stochastic gradient decent will only use a randomly selected batch of training data to train the algorithm. For our purposes we only have two entries for training data and thus the stochastic gradient descents will use either or to mimic the method in question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our cost function\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "h(x; \\theta) &= \\langle \\theta^Tx \\rangle \\\\\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2}(h(x^{(i)}; \\theta)-y^{(i)})^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Only using $(x^{(1)},y^{(1)})$, we arrive at:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4}\\theta_0^2 +\\frac{1}{4}\\theta_0\\theta_1 + \\frac{3}{2}\\theta_0\\theta_2 + \\frac{1}{16}\\theta_1^2 + \\frac{3}{4}\\theta_1\\theta_2 + \\frac{9}{4}\\theta_2^2 -\\frac{5}{2}\\theta_0 - \\frac{5}{4}\\theta_1 - \\frac{15}{2}\\theta_2 +\\frac{25}{4}\\\\\n",
    "$$\n",
    "\n",
    "Calculating the partial derivatives with respect to $\\theta_i$ vector we get:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} &= \\frac{1}{2}\\theta_0 + \\frac{1}{4}\\theta_1 + \\frac{3}{2}\\theta_2 -\\frac{5}{2}\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} &= \\frac{1}{4}\\theta_0 + \\frac{1}{8}\\theta_1 + \\frac{3}{4}\\theta_2 -\\frac{5}{4} \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_2} &= \\frac{3}{2}\\theta_0 + \\frac{3}{4}\\theta_1 + \\frac{9}{2}\\theta_2 - \\frac{15}{2} \n",
    "\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus our columns vector is \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\begin{bmatrix}\n",
    "\\frac{1}{2}\\theta_0 + \\frac{1}{4}\\theta_1 + \\frac{3}{2}\\theta_2 -\\frac{5}{2}\\\\\n",
    "\\\\\n",
    "\\frac{1}{4}\\theta_0 + \\frac{1}{8}\\theta_1 + \\frac{3}{4}\\theta_2 -\\frac{5}{4}\\\\\n",
    "\\\\\n",
    "\\frac{3}{2}\\theta_0 + \\frac{3}{4}\\theta_1 + \\frac{9}{2}\\theta_2 - \\frac{15}{2}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our cost function\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "h(x; \\theta) &= \\langle \\theta^Tx \\rangle \\\\\n",
    "\\\\\n",
    "J(\\theta) &= \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2}(h(x^{(i)}; \\theta)-y^{(i)})^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Only using $(x^{(2)},y^{(2)})$, we arrive at:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{4}\\theta_0^2 +\\frac{1}{10}\\theta_0\\theta_1 - \\frac{1}{2}\\theta_0\\theta_2 + \\frac{1}{100}\\theta_1^2 - \\frac{1}{10}\\theta_1\\theta_2 + \\frac{1}{4}\\theta_2^2 -\\theta_0 - \\frac{1}{5}\\theta_1 +\\theta_2 +1\\\\\n",
    "$$\n",
    "\n",
    "Calculating the partial derivatives with respect to $\\theta_i$ vector we get:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} &= \\frac{1}{2}\\theta_0 + \\frac{1}{10}\\theta_1 + \\frac{1}{2}\\theta_2 -1\n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} &= \\frac{1}{10}\\theta_0 + \\frac{1}{50}\\theta_1 - \\frac{1}{10}\\theta_2 -\\frac{1}{5} \n",
    "\\\\\n",
    "\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_2} &= -\\frac{1}{2}\\theta_0 - \\frac{1}{10}\\theta_1 + \\frac{1}{2}\\theta_2 +1\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Thus our columns vector is \n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_i} = \\begin{bmatrix}\n",
    "\\frac{1}{2}\\theta_0 + \\frac{1}{10}\\theta_1 + \\frac{1}{2}\\theta_2 -1\\\\\n",
    "\\\\\n",
    "\\frac{1}{10}\\theta_0 + \\frac{1}{50}\\theta_1 - \\frac{1}{10}\\theta_2 -\\frac{1}{5} \\\\\n",
    "\\\\\n",
    "-\\frac{1}{2}\\theta_0 - \\frac{1}{10}\\theta_1 + \\frac{1}{2}\\theta_2 +1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
