{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by showing the Jensen inequality. Where $f(x) = -\\log x$ since it is concave for $x > 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "KL &= \\int p(x) \\left[ -\\log \\frac{q(x)}{p(x)} \\right]dx = \\mathbb{E}_p[-\\log g(x)] \\\\\n",
    "\\\\\n",
    "&\\ge -\\log \\mathbb{E}_p[g(x)] = -\\log \\int p(x) \\frac{q(x)}{p(x)}dx = -\\log \\int q(x)dx = -\\log(1) = 0\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this must hold true, $g(x) = \\frac{p(x)}{q{x}} = c$ but since we are able to see that if that we end up with just taking the integral of one PDF over the same domain. We can see that it must not equal a constant but actually, $g(x) = \\frac{p(x)}{q(x)} = 1$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "||x - z_1u_1 - z_2u_2||^2 &= (x - z_1u_1 - z_2u_2)^T(x - z_1u_1 - z_2u_2)\\\\\n",
    "\\\\\n",
    "& = (x^T - (z_1u_1)^T - (z_2u_2)^T)(x - z_1u_1 - z_2u_2)\\\\\n",
    "\\\\\n",
    "&= x^Tx - x^Tz_1u_1 - x^Tz_2u_2\\\\\n",
    "&- (z_1u_1)^Tx + <z_1u_1, z_1u_1> + <z_1u_1, z_2u_2>\\\\\n",
    "&- (z_2u_2)^Tx + <z_2u_2, z_1u_1> + <z_2u_2, z_2u_2>\\\\\n",
    "\\\\\n",
    "&= x^Tx - x^Tz_1u_1 - x^Tz_2u_2\\\\\n",
    "&- (z_1u_1)^Tx + (x^Tu_1)^2<u_1, u_1> + (x^Tu_1)(x^Tu_2)<u_1, u_2>\\\\\n",
    "&- (z_2u_2)^Tx + (x^Tu_2)^2<u_2, u_2> + (x^Tu_1)(x^Tu_2)<u_1, u_2>\\\\\n",
    "\\\\\n",
    "&= x^Tx - x^Tz_1u_1 - x^Tz_2u_2\\\\\n",
    "&- (z_1u_1)^Tx + (x^Tu_1)^2\\\\\n",
    "&- (z_2u_2)^Tx + (x^Tu_2)^2\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the following to be true.\n",
    "\n",
    "$$\n",
    "x = u_j(u_j^Tx) = \\frac{u_ju_j^T}{u_j^Tu_j}x\n",
    "$$\n",
    "\n",
    "Thus we state the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "||x - z_1u_1 - z_2u_2||^2 &= x^Tx - x^T(x^Tu_1)u_1 - x^T(x^Tu_2)u_2\\\\\n",
    "&- ((x^Tu_1)u_1)^Tx + (x^Tu_1)^2\\\\\n",
    "&- ((x^Tu_2)u_2)^Tx + (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "&= ||x||^2 - \\frac{u_1u_1^T}{u_1^Tu_1}x\\frac{u_1u_1^T}{u_1^Tu_1}x - \\frac{u_2u_2^T}{u_2^Tu_2}x\\frac{u_2u_2^T}{u_2^Tu_2}x\\\\\n",
    "&- \\frac{u_1u_1^T}{u_1^Tu_1}x\\frac{u_1u_1^T}{u_1^Tu_1}x + (x^Tu_1)^2\\\\\n",
    "&- \\frac{u_2u_2^T}{u_2^Tu_2}x\\frac{u_2u_2^T}{u_2^Tu_2} + (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "&= ||x||^2 - <u_1(u_1^Tx),u_1(u_1^Tx)> - <u_2(u_2^Tx),u_2(u_2^Tx)>\\\\\n",
    "&- <u_1(u_1^Tx),u_1(u_1^Tx)> + (x^Tu_1)^2\\\\\n",
    "&- <u_2(u_2^Tx),u_2(u_2^Tx)> + (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "&= ||x||^2 - (x^Tu_1)^2<u_1, u_1> - (x^Tu_2)^2<u_2, u_2>\\\\\n",
    "&- (x^Tu_1)^2<u_1, u_1> + (x^Tu_1)^2\\\\\n",
    "&- (x^Tu_2)^2<u_2, u_2> + (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "&= ||x||^2 - (x^Tu_1)^2 - (x^Tu_2)^2\\\\\n",
    "&- (x^Tu_1)^2 + (x^Tu_1)^2\\\\\n",
    "&- (x^Tu_2)^2 + (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "& = ||x||^2 - (x^Tu_1)^2 - (x^Tu_2)^2\\\\\n",
    "\\\\\n",
    "& = ||x||^2 - [(x^Tu_1)^2 + (x^Tu_2)^2]\\\\\n",
    "\\\\\n",
    "& = ||x||^2 - \\sum_{j=1}^2(x^Tu_j)^2\\\\\n",
    "\\\\\n",
    "& = ||x||^2 - \\sum_{j=1}^2(z_j)^2\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonality: \n",
    "\n",
    "They both deal with finding a hyperplane that will seperate the classes correctly within some error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differences: \n",
    "\n",
    "In LDA, the seperating hyperplane seperates the means of the samples in each class while in SVM, finds the hyperplane with the maximum of the minimum of distance between the hyperplane to the nearest point in each class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
