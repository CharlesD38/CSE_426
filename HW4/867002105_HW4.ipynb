{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\\tableofcontents \n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given the two training examples $x^{(1)} = [2, 1]^T \\ $, $y^{(1)} = 1$ and $x(2) = [1,-1]^T \\ $, $y^{(2)} = -1 \\ $, First write down the SVM primal problem and plug in the data (so that $w$ and $b$ are variables). Then construct the Lagrangian function $L(w, b,\\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Primal Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\min_{w,b} \\quad & f(w) = \\frac{1}{2} \\lVert w \\rVert ^2\\\\\n",
    "\\\\\n",
    "\\text{s.t} \\quad & \\left[w^T \\begin{bmatrix} 2 \\\\\n",
    "1\\end{bmatrix} + b \\right] -1 \\geq 0 \\\\\n",
    "\\\\\n",
    "\\quad & -\\left[w^T \\begin{bmatrix} 1 \\\\\n",
    "-1\\end{bmatrix} + b \\right] -1 \\geq 0 \\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lagrangian Function $L(w,b,\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "L(w,b,\\lambda) & = f(w) - \\sum_{i-1}^m \\lambda_ig_i(w)\\\\\n",
    "\\\\\n",
    "& = \\frac{1}{2} \\lVert w \\rVert ^2 - \\sum_{i-1}^m \\lambda_i(y^{(i)}(w^Tx^{(i)}+b)-1)\\\\\n",
    "\\\\\n",
    "& = \\frac{1}{2} \\lVert w \\rVert ^2 - \\left[\n",
    "    \\lambda_1\\left[\\left[w^T \\begin{bmatrix} 2\\\\\n",
    "1\\end{bmatrix} + b \\right] -1\\right] - \\lambda_2\\left[\\left[w^T \\begin{bmatrix} 1\\\\\n",
    "-1\\end{bmatrix} + b \\right] -1\\right]\n",
    "\\right]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write down the KKT conditions of the above problem. Simplify the partial derivatives and write down the elements of the vectors (i.e., don't vectorize the conditions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KKT Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{1}{2} - \\lambda_1x^{(1)} + \\lambda_2x^{(2)}  & = 0\\\\\n",
    "\\\\\n",
    "\\lambda_2 - \\lambda_1 &= 0 \\\\ \n",
    "\\\\ \n",
    "\\lambda_1 &\\ge 0\\\\\n",
    "\\\\\n",
    "\\lambda_2 &\\ge 0\\\\\n",
    "\\\\\n",
    "y^{(1)}(w_1x_1^{(1)}+w_2x_2^{(1)}+b)-1 &\\ge 0\\\\\n",
    "\\\\\n",
    "y^{(2)}(w_1x_1^{(2)}+w_2x_2^{(2)}+b)-1 &\\ge 0\\\\\n",
    "\\\\\n",
    "\\lambda_1(y^{(1)}(w_1x_1^{(1)}+w_2x_2^{(1)}+b)-1) &= 0\\\\\n",
    "\\\\\n",
    "\\lambda_2(y^{(2)}(w_1x_1^{(2)}+w_2x_2^{(2)}+b)-1) &= 0\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\frac{1}{2} - \\lambda_1\\begin{bmatrix} 2\\\\\n",
    "1\\end{bmatrix} + \\lambda_2\\begin{bmatrix} 1\\\\\n",
    "-1\\end{bmatrix}  & = 0\\\\\n",
    "\\\\\n",
    "\\lambda_2 - \\lambda_1 &= 0 \\\\ \n",
    "\\\\ \n",
    "\\lambda_1 &\\ge 0\\\\\n",
    "\\\\\n",
    "\\lambda_2 &\\ge 0\\\\\n",
    "\\\\\n",
    "2w_1+w_2+b-1 &\\ge 0\\\\\n",
    "\\\\\n",
    "-w_1-w_2-b+1 &\\ge 0\\\\\n",
    "\\\\\n",
    "\\lambda_1(2w_1+w_2+b-1) &= 0\\\\\n",
    "\\\\\n",
    "\\lambda_2(-w_1-w_2-b+1) &= 0\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuing the above problem. Write down the dual SVM optimization problem with linear kernel. (Hint: No need to derive the dual problem, but just plug the data in the dual problem in the lecture note).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "\\max_\\lambda \\quad & L(\\lambda) = \\lambda_1 + \\lambda_2 - \\frac{5}{2}\\\\\n",
    "\\\\\n",
    "\\text{s.t.} \\quad & \\lambda_1-\\lambda_2 = 0\\\\\n",
    "\\\\\n",
    "& \\lambda_1, \\ \\lambda_2 \\ge 0\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue the above question. Given the dual parameters $\\alpha = [0.5, 1]$, which may not be optimal, recover the primal parameter $w$ (using the first-order condition of the KKT condition of SVM). Use linear kernel and solve for $b$ by noting that any support vector on the margin should have $y^{(i)}(w^Tx^{(i)}+b) = 1$. Then use the dual parameters to classify the test example $x = [1, 1]^T$ by computing the classifier output $h(x;\\alpha)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "w &= \\lambda_1y^{(1)}x^{(1)}+\\lambda_2y^{(2)}x^{(2)}\\\\\n",
    "\\\\\n",
    "&= 0.5 \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\\\\n",
    "\\\\\n",
    "&= \\begin{bmatrix} 0 \\\\ 1.5 \\end{bmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "y^{(1)}(w_1x_1^{(1)}+w_2x_2^{(1)}+b) &= 1\\\\\n",
    "\\\\\n",
    "1.5 + b &= 1\\\\\n",
    "\\\\\n",
    "y^{(2)}(w_1x_1^{(2)}+w_2x_2^{(2)}+b) &= 1\\\\\n",
    "\\\\\n",
    "1.5 - b &=1\\\\\n",
    "\\\\\n",
    "1.5 - b &= 1.5 + b\\\\\n",
    "\\\\\n",
    "b=0\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "h(x;\\lambda) & = \\sum_{i=1}^m \\lambda_i y^{(i)} (x^{(i)})^T(x) +b\\\\\n",
    "\\\\\n",
    "&= 0.5 [2, 1] [1, 1]^T + [-1, 1] [1, 1]^T\\\\\n",
    "\\\\\n",
    "&= 1.5\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Graduate only) Prove that there is a function  $\\psi([x_1, x_2])$, that maps from the input space $\\mathbb{R}^2$ to a higher dimensional space $\\mathbb{R}^n$, $n > 2$, so that the polynomial kernel $k(x, z) = (\\langle x, z \\rangle +1)^2$ can be written as $\\langle\\psi(x), \\psi(z) \\rangle$.\n",
    "\n",
    "Hints: refer to PRML Eq. (6.12) for help and write down the formula of the function $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are asked to prove that some function $\\psi([x_1,x_2]$ maps from the $\\mathbb{R}^2$ to a larger dimensional space such that polynomial kernal function $k(x,z) = (\\langle x, z \\rangle + 1)^2$ can be written as $\\langle \\psi(x), \\psi(z) \\rangle$. \n",
    "\n",
    "We will start by multiplying the singular inner terms of the two dimensional column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "k(x,z) &= (\\langle x, z \\rangle + 1)^2\\\\\n",
    "\\\\\n",
    "&= (x^T z + 1)^2\\\\\n",
    "\\\\\n",
    "&= (x_1z_1+X_2z_2 +1)^2\\\\\n",
    "\\\\\n",
    "&= x_1^2z_1^2+2x_1z_1x_2z_2 +2x_1z_1 + 2x_2z_2 + x_2^2z_2^2 + 1\\\\\n",
    "\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can see the mapping function $\\psi$ will have the following characteristics\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& (x_1^2, \\sqrt{2}x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, x_2^2, 1)^T(z_1^2, \\sqrt{2}z_1z_2, \\sqrt{2}z_1, \\sqrt{2}z_2, z_2^2, 1)\\\\\n",
    "\\\\\n",
    "&= \\psi(x)^T\\psi(z)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily see that we have mapped the column vector from one input space ($\\mathbb{R}^2$) to a higher order dimensional space. The $\\psi(x)$ function (in this two dimensional input space example) maps to ($\\mathbb{R}^6$) space. This function performs operations on the inner values of the input vector and thus we can see that the inner product $\\langle \\psi(x), \\psi(z) \\rangle$ is the same as $k(x,z) = (\\langle x, z \\rangle + 1)^2$ and we have already shown that $\\psi$ maps to a higher dimensional space.\n",
    "\n",
    "To solidify, given the two input space vector $x = [x_1, x_2]$. The function $\\psi(x) = [x_1^2, \\sqrt{2}x_1x_2, \\sqrt{2}x_1, \\sqrt{2}x_2, x_2^2, 1]$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
