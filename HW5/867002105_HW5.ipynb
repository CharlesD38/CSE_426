{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\pagebreak\n",
    "\\tableofcontents \n",
    "\\pagebreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the Lagrangian function of the primal SVM be $$L(w, \\lambda) = f(w) - \\sum_{i=1}^m \\lambda_ig_i(w) = f(w) - \\lambda^Tg(w)$$ with the multipliers $\\lambda \\succeq  0$ and inequality constraints $g(w) \\succeq 0$. Prove the weak duality property: $$ \\max_{\\lambda \\succeq  0} \\min_w L(w, \\lambda) \\leq  \\min_w \\max_{\\lambda \\succeq  0}  L(w, \\lambda) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given the following primal problem:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\max& \\quad L(w, \\lambda) =  f(w) - \\lambda^Tg(w)\\\\\n",
    "\\\\ \n",
    "\\text{s.t.}& \\quad \\lambda \\succeq 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We set $\\lambda^{'}$ to be the optimal solution to the dual. Plugging this into the same function where all else is equal. We can clearly see that the subtractive term could become positive at certain W. This would then yeild the result $L(w,\\lambda^{'}) \\leq \\max_{\\lambda \\succeq 0} L(w, \\lambda)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add in the primal constraint. $\\min_w L(w, \\lambda^{'}) \\leq \\min_w \\max_{\\lambda \\succeq  0}  L(w, \\lambda)$. To prove this one simply needs to see that we are maximizing the objective without the primal constraint and this will yield $\\min_w  L(w)$ as it is not a function of $\\lambda$ while $\\lambda^{'}$ is the optimal soution already as well as minimizing a convex function. We can clearly see that the first relationship holds. These inequalities show that $\\max_{\\lambda \\succeq  0} \\min_w L(w, \\lambda) \\leq \\min_w \\max_{\\lambda \\succeq  0} L(w, \\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by using the hyperplane $y=x$ in point slope form. Finding the $w^{\\perp}$ to satisfy $w^Tx = 0$ will give us $ w = [-1, 1]^T$. We now set up our maximum margin linear classifier $\\hat{h}(x;w)$. This classifier will also be used to have the new values used for the geometric margin calculations $(y^{(i)})$. Also, through linear algebra, one can see that the perpendicular distance aka the margin, is $\\frac{\\sqrt{2}}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h(x;w) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } -x_1^{(i)} + x_2^{(i)} > 0 \\\\\n",
    "-1 & \\text{if } -x_1^{(i)} + x_2^{(i)} \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that, based on our new classifier, that $y^{(1)}, y^{(2)} = 1, -1$ respectively. The change creates a correct margin calulation as well as properly classiying the trainging examples. Shown below:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\frac{y^{(1)}(w_2)}{\\left\\Vert w \\right\\Vert} &= \\frac{\\sqrt{2}}{2} = \\frac{y^{(2)}(w_1)}{\\left\\Vert w \\right\\Vert}\\\\\n",
    "\\\\\n",
    "\\frac{y^{(1)}}{\\sqrt{2}} &= \\frac{\\sqrt{2}}{2} = \\frac{-y^{(2)}}{\\sqrt{2}}\\\\\n",
    "\\\\\n",
    "\\frac{1}{\\sqrt{2}} &= \\frac{\\sqrt{2}}{2} = \\frac{1}{\\sqrt{2}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This shows we have correctly formulated the classifier for our training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "L(h) &= \\mathbb(E)[\\mathbb{1}[h(x)\\neq(x)]]\\\\\n",
    "\\\\\n",
    "& = P\\left(\\frac{2\\pi}{3} < \\theta < \\frac{5\\pi}{4} \\right) + P\\left(\\frac{5\\pi}{4} < \\theta < \\frac{7\\pi}{4} \\right)\\\\\n",
    "\\\\\n",
    "& = \\left(\\frac{\\pi}{2}\\right)\\frac{1}{2\\pi} + \\left(\\frac{\\pi}{4}\\right)\\frac{1}{2\\pi}\\\\\n",
    "\\\\\n",
    "& = \\frac{1}{8} + \\frac{1}{4} = \\frac{3}{8}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "L(x: \\{x^{(i)}, y^{(i)} \\}_{i=1}^m) & = \\frac{1}{2}\\sum_{i=1}^m (y^{(i)} - \\langle w, x^{(i)} \\rangle)^2\\\\\n",
    "\\\\\n",
    "& = \\frac{1}{2}\\left[\\sum_{i=1}^m (y^{(i)})^2 -2\\sum_{i=1}^m y^{(i)}\\langle w, x^{(i)} \\rangle + \\sum_{i=1}^m \\langle w, x^{(i)} \\rangle^2   \\right] \\\\\n",
    "\\\\\n",
    "& = \\frac{1}{2}\\left[\\sum_{i=1}^m (y^{(i)})^2 - 2\\sum_{i=1}^m \\sum_{j=1}^m y^{(i)} \\alpha_j K_{ij} + \\left(\\sum_{i=1}^m  \\alpha_j K_{ij}\\right)^2  \\right]\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8377259de029fbe3469e5825885a3984679ef58677fe54558bfe80e0473ceee4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
